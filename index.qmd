---
title: "Capstone - Principal Component Analysis"
author: "Nick Belgau, Oscar Hernandez Mata"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Across diverse industries and data science applications, Principal Component Analysis (PCA) offers a powerful solution to extracting valuable insights from complex datasets. The primary objective of PCA is to reduce the number of dimensions in the dataset while retaining most of the original information. This is directly applicable to big data scenarios where it is not feasible to rely on univariate analysis to find patterns, and redundancy in the data leads to inflated standard errors and poor model stability [@rahayu2017application].  

Developed by Karl Pearson in 1901, PCA utilizes linear algebra to transform the original dataset to a lower dimensional vector space of uncorrelated variables known as principal components. These are linear combinations of the original variables, and represent the information in a new coordinate system with axes aligned to the directions of maximum variability [@joshi2020prediction]. Most of the data lies in this new feature subspace, where variance is used to measure the amount of information it contains. The quality of this subspace is assessed by comparing its variance to the total variance of the entire dataset, helping PCA identify the main structure in the data [@marukatat2023tutorial].  

PCA is often used during exploratory data analysis because this technique enables graphical visualization of the dataset. This can reveal unexpected relationships between the original variables that would otherwise be challenging to identify [@johnson2023applied]. By reducing the dimensionality, PCA simplifies the data structure, making it easier to interpret and analyze. As a result, trends, patterns, and outliers can be identified in the new reduced-dimension dataset [@richardson2009pca].  

Additionally, PCA is a powerful tool to address the curse of dimensionality and the main problem areas associated with high dimensionality: data sparsity, multicollinearity, and overfitting [@altman2018curse]. By projecting the data onto the principal components, the density of data points is increased in the new vector space which makes it easier to detect patterns and reduce noise. Multicollinearity is mitigated in the process because the principal components are uncorrelated to each other which improves stability and performance of the predictive models [@bharadiya2023tutorial]. With dimensionality reduction, models become simpler which improves generalization and reduces overfitting.  

These benefits become valuable in many real-world applications, including image compression and signal processing. Lengthy image transfer over the internet can be improved by factoring image pixel matrices to extract the main patterns with minimal information loss [@ng2017pca]. For image processing tasks and particularly in edge-computing applications, PCA can file size by 60% with minimal information loss [@ali2024dimensionality]. This is especially useful for real-time quality control systems with deployed ML models in advanced manufacturing environments. For signal processing, PCA can be used to identify the underlying structure and relevant features in signals amongst the noise.  

In machine learning pipelines, PCA is often employed because reducing the number of dimensions decreases computational complexity, lowers memory requirements, and enhances algorithm efficiency. Additionally, PCA's feature extraction capabilities allow for a better understanding and interpretation of the underlying data structure by identifying the most influential variables [@bharadiya2023tutorial]. It effectively filters out noise and irrelevant variations, enhancing the signal-to-noise ratio and improving the performance of subsequent analyses. Using PCA in both the training and deployment stages ensures consistent data transformation, maintains model stability, and boosts prediction speed by working with fewer, more informative features.  

As with every technique, PCA has a few inherent limitations. It assumes linearity which means that if nonlinear relationships exist, PCA will not as effective because it may fail to capture the underlying structure in the data. However, modern variations such as Kernal PCA seeks to address this [@marukatat2023tutorial]. Interpretability is another considerable downside because the resulting principal components are combinations of the original variables. PCA is also sensitive to outliers, which can distort the variance-covariance structure and affect the quality of dimensionality reduction. However, there have been improvements to address outlier sensitivity such as Robust PCA which decomposes the data into a low-rank matrix and sparse matrix to separate signal and noise [@bharadiya2023tutorial].  

Additionally, PCA results in some degree of information loss, as the lower-dimensional representation will not retain all details from the original data. More specifically, in image compression, PCA can lead to loss of fine details and subtle features in images, especially when a high compression rate is applied. In addition, the computation of eigenvectors and eigenvalues, especially for large images, can be resource intensive and time-consuming [@do2012pca]. Lastly, selecting the optimal number of components to retain is subjective; choosing too few can lead to significant information loss, while choosing too many can lead to overfitting or unnecessary complexity.  

Despite these drawbacks, PCA is crucial in data science for its ability to simplify complex datasets, reduce computational costs, and enhance the interpretability of data by focusing on the most significant features. It offers a powerful tool for improving model performance, reducing noise, and uncovering hidden patterns, making it indispensable for efficient and insightful data analysis.





## Methods  

### Linear Algebra Foundations  

Principal Component Analysis (PCA) simplifies multivariate data by transforming the original potentially correlated variables into a smaller set of uncorrelated variables called principal components. The aim of PCA is to reduce the dataset's dimensionality while preserving as much variability as possible. Because the variables will often have different scales or units, standardization is crucial to ensure each variable contributes equally to the analysis. Principal components are obtained by identifying the directions of maximum variability in the data. These components are orthogonal and uncorrelated, with each one capturing a specific amount of the data's variance. This process creates a new coordinate system that simplifies the data structure while retaining its essential characteristics [@johnson2023applied].

While different methods can be used to determine the principal components, Singular Value Decomposition (SVD) is the most common due to its computational efficiency and numerical stability which makes it a popular choice for popular programming packages. SVD decomposes the data into three simpler matrices, making it possible to handle large datasets effectively. This decomposition allows for a more straightforward calculation of the principal components without explicitly computing the covariance matrix, which can be computationally expensive and numerically unstable. The ability to handle sparse and dense matrices further enhances its versatility and efficiency in diverse applications. These advantages make SVD a preferred method in popular programming packages, ensuring that PCA is performed quickly and accurately.

### Overview of Algorithm with SVD:  

1. **Standardize the data**  
   Ensure each variable contributes equally by having a mean of zero and variance of one. This requires that the variables are continuous.
   $$
   X_{\text{standardized}} = \frac{X - \mu}{\sigma}
   $$
   $\mu$: mean, $\sigma$: standard deviation of each variable

2. **Perform Singular Value Decomposition (SVD)**  
  Decompose the standardized data matrix into three matrices $U$, $\Sigma$, and $V^T$.
  $$
  X_{\text{standardized}} = U \Sigma V^T
  $$
  $ùëà$ and $V^T$ are orthogonal matrices, so therefore $U$ and $V$ are also orthogonal. The diagonal matrix of singular values is represented by $\sigma$ with values $\sigma_i$ that are naturally sorted in descending order. Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other in the transformed feature space. The number of PC's initially generated are equivalent to the number of variables initially provided in the dataset.

3. **Selection of principal components**  
  The explained variance of each PC is represented by:
  $$
  \text{variance\_explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
  $$
  For each singular value $\sigma_i$ in the matrix of singular values $\Sigma$, compute $\sigma_i^2$ and calculate the individual variance. The cumulative explained variance target is specified (typically 95%) as a criteria for PC selection. Determine the number of components needs to reach the desired cumulative explained variance.

4. **Transform the data**  
  Project the standardized data onto the selected principal components.
  $$
  X_{\text{transformed}} = X_{\text{standardized}} V_{\text{selected}}
  $$
  where $V_{\text{selected}}$ contains the first number of components columns of $V$. Once the data is transformed onto the selected PC's, it is effectively reduced in dimensionality while retaining most of its original variability.




### Assumptions and Practical Testing  

This section covers the primary assumptions of PCA and details how they can be tested in real-world applications. A more accurate name for this section might be *requirements* since the effectiveness of PCA relies on satisfying these points.

#### 1. Linearity
**Assumption**: PCA assumes that resulting principal components are linear combinations of the original variables. Nonlinear relationships may lead to low covariance values which can lead to an undervalued representation of their signficance.

**Testing**: Initial checks with scatter plots and correlation matrices can help identify linear relationships between variable pairs. While common, these methods require manual inspection which introduces the potential for the researcher to miss subtler non-linear patterns. Supplementing with statistical tests for linearity can provide a more robust assessment. Transformations can be applied to specific non-linear variables or alternative PCA methods can be utilized such as Kernal PCA [@marukatat2023tutorial].


##### 2. Continuous data
**Assumption**: To calculate principal components, data should be on a continuous scale: either interval or ratio. It should be mentioned that despite being ordinal, Likert Scales are often used because the distance between scale points are assumed to be approximately equivalent.  

**Testing**: Reviewing column data types and value counts are practical methods for testing. Simple transformations can be applied to categorical data.

#### 3. Data standardization 

**Assumption**: There are three critical preprocessing steps: scaling, mean-centering, and outlier handling. Scaling standardizes the variance of each variable to ensure equal contributions. Mean-centering has a similar impact: ensuring that the principal components capture the true direction of maximum variance. Outliers can also distort the principal components, so they should be identified and handled appropriately. Together these methods prevent variables with larger measured values from dominating the principal components and skewing results.

**Testing**: Data will usually not arrive in a condition that meets this requirement. Luckily, statistical packages in common programming languages offer simple methods to scale and mean-center data. Outliers can be detected using a variety of distribution plots, but automated methods could be employed for larger datasets which is beyond the scope of this paper.  

! need to add Varimax, and what loadings are 


## Applications: Analysis and Results

### Application 1: General Dimensionality Reduction
Load required ibaries:
```{r}
library(readxl)
library(skimr)
library(dplyr)
library(ggplot2)
library(GGally)
library(corrplot)
library(scales)
library(DescTools)
library(reshape2)
library(factoextra)
library(readr)
library(kableExtra)
library(rrcov)
library(psych)
```

1.  **Dataset Description**

The dataset contains demographic, health, and environmental metrics for counties in Florida (FL) and Georgia (GA). Each row represents data for a specific county, and the selected columns are:

- County: Name of the county.
- State: State abbreviation (FL or GA).
- obesity_age_adj: Age-adjusted obesity rate.
- Smoking_Rate: Rate of smoking within the population.
- physical_inactivity: Rate of physical inactivity.
- Diabetes: Diabetes prevalence rate.
- Heart_Disease: Heart disease prevalence rate.
- Cancer: Cancer prevalence rate.
- NATA_Cancer_11: National-Scale Air Toxics Assessment (NATA) cancer risk for 2011.
- Poverty_Percent: Percentage of the population living below the poverty line.

```{r}
data_raw <- read_csv("data/Alz_mortality_data_complete_with_state_csv.csv")
```


The dataset has 3109 observations and 66 variables.

2.  **Filtered Dataset and Description**

```{r}
selected_columns <- c(
  "County", "State", "alz_ageadj_rate", "mental_distress", "Med_age", "sixtyfiveandup", "obesity_age_adj", "Smoking_Rate", "physical_inactivity", "Diabetes", "Heart_Disease", "Cancer", "Glyphosates", "NATA_Cancer_11", "Fine_PM_2_5", "Mercury_TPY", "Lead_TPY", "Percent_Less_than_HS", "Percent_with_Bachelors", "Pop_Dens", "per_rural", "Food_index", "Households_2up", "Household_3up", "Household_4up", "Poverty_Percent", "Atrazine_High_KG", "SUNLIGHT", "NATA_resp_11", "age_rate", "Gly_sqkm"
)

data <- USHealth %>%
  filter(State %in% c("FL")) %>%
  select(all_of(selected_columns))

data <- data %>%
  select(-County, -State)
```

We filtered the dataset to analyze selected variables in the southern state of Florida. We remove the columns to denote which county and state the data is from.
Columns were selected to remove demographic fields related to gender, sex, and race and ambigious fields related to grouping.


```{r}
sapply(data, class)
```

```{r}
sapply(data, function(x) length(unique(x)))
```

Cursory check to inspect nulls and scales.
```{r}
skim(data)
```
We note that [mental_distress] is ordinal but we will treat it as continuous.  

**Standardize the data**
```{r}
data <- scale(data)
```


After filtering the dataset, our new dataset contains 226 observations and 10 variables.

3.  **Correlation analysis**

```{r}
cor_matrix <- cor(data, use = "complete.obs")
# print(cor_matrix)
```

Most of the variables in our new dataset reveal a high correlation, which can prompt PCA to express them in a small number of principal components.

4.  **Linarity Analysis**

```{r}
data <- as.data.frame(data)

custom_plot <- ggpairs(
  data,
  upper = list(continuous = wrap("cor", size = 1.5, colour = "#4E4E50")), 
  # Smaller correlation numbers
  lower = list(continuous = wrap("points", alpha = 0.5, size = 0.8, colour = "#4E4E50")),
  diag = list(continuous = wrap("barDiag", fill = "#4E4E50", colour = "#4E4E50")),
  axisLabels = 'show'
) +
  theme(
    panel.background = element_rect(fill = "white"), # Background of the plot
    plot.background = element_rect(fill = "white"),  # Background of the panel
    legend.background = element_rect(fill = "white"), # Background of the legend
    panel.grid.major = element_blank(), # Major grid lines
    panel.grid.minor = element_blank(), # Minor grid lines
    axis.text = element_text(color = "#4E4E50", size = 6), # Smaller axis text
    axis.title = element_text(color = "#4E4E50", size = 8), # Smaller axis titles
    strip.background = element_rect(fill = "white"),
    strip.text = element_text(color = "#4E4E50", size = 8) # Smaller variable names
  )

custom_plot
```

! Issues with code
```{r}
# par(col.axis="#8B814C", col.lab="#8B814C", col.main="#8B814C", col.sub="#8B814C", pch=20, col="#8B814C", bg="transparent")
# DescTools::PlotPairs(sapply(filtered_data, function(x) jitter(x, 5)), 
#                      g=filtered_data$Smoking_Rate,
#                      col=scales::alpha("#8B814C", 0.1), 
#                      col.smooth="#8B814C"
#                      )
```

```{r}
# # Convert the data to long format
# melted_data <- melt(filtered_data)


# # Define a function to create scatter plots with smooth lines
# plot_pair <- function(df, x_var, y_var) {
#   ggplot(df, aes_string(x = x_var, y = y_var)) +
#     geom_point(alpha = 0.5) +
#     geom_smooth(method = "loess", color = "#8B814C", se = FALSE) +
#     theme_minimal() +
#     labs(title = paste("Scatter Plot of", x_var, "vs", y_var),
#          x = x_var,
#          y = y_var)
# }

# # Get the variable names
# vars <- colnames(filtered_data)

# # Create scatter plots for each pair of variables
# for (i in 1:(length(vars) - 1)) {
#   for (j in (i + 1):length(vars)) {
#     print(plot_pair(filtered_data, vars[i], vars[j]))
#   }
# }

```

The correlation matrix plots show enough of a relationship between variables to meet the assumption of linearity.

5.  **Outliers analysis**

```{r}
melted_data <- melt(filtered_data)

# Create boxplots for each variable
ggplot(melted_data, aes(x = variable, y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplots for Each Variable",
       x = "Variable",
       y = "Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

6.  **Transformation and Outliers.**

```{r}
filtered_data_transformed <- log(filtered_data + 1)
```

```{r}
melted_data2 <- melt(filtered_data_transformed)

# Create boxplots for each variable
ggplot(melted_data2, aes(x = variable, y = value)) +
  geom_boxplot(outlier.colour = "red", outlier.shape = 16, outlier.size = 2) +
  theme_minimal() +
  labs(title = "Boxplots for Each Variable",
       x = "Variable",
       y = "Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The dataset revealed some outliers, thus we conducted a log transformation to normalize the distribution making the dataset more suitable for analysis without the skewness caused by outliers.

! there is an issue here!! too many data points removed

7.  **PCA with Varimax Rotation results**

```{r}
# # Perform PCA with Varimax rotation
# pca_with_rotation <- principal(filtered_data_transformed, nfactors = ncol(filtered_data_transformed), rotate = "varimax", scores = TRUE)

# # Summary of PCA with rotation
# print(pca_with_rotation)


# # Plot the proportion of variance explained
# fa.diagram(pca_with_rotation)


```

For PCA we conducted Varimax rotation, since it helps to get clearer and more distinct components that are easier to interpret by adjusting the loadings and maximizing the sum of variances of the squared loadings.





Need to expand...  
Screeplot
```{r}



```
If the plot shows nonlinear relationships, need to examine

Biplot
In one of these applications, discuss how PCA aids in building more robust statistical models by reducing multicollinearity and enhancing model interpretability.
Compare VIF (variance inflation factor) before and after PCA.


### Application 2: Feature Extraction  
continuation of application 1 but go into ML part

### Application 3: Image Compression  

New example.
Probably dont need to standardize the data. 
No scree plot or biplot because pixels.
Results to focus on memory reduction and maintaining equivalent level of performance.
This will be similar to appl 1 + 2 but without going into as much detail for techniques already demonstrated.

## Conclusion  

Summary, reflections, future discissions

