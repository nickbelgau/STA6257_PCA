---
title: "Practical Implementations of Principal Component Analysis"
author: "Nick Belgau, Oscar Hernandez Mata"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Across diverse industries and data science applications, Principal Component Analysis (PCA) offers a powerful solution to extracting valuable insights from complex datasets. The primary objective of PCA is to reduce the number of dimensions in the dataset while retaining most of the original information. This is directly applicable to big data scenarios where it is not feasible to rely on univariate analysis to find patterns, and redundancy in the data leads to inflated standard errors and poor model stability [@rahayu2017application].  

Developed by Karl Pearson in 1901, PCA utilizes linear algebra to transform the original dataset to a lower dimensional vector space of uncorrelated variables known as principal components. These are linear combinations of the original variables, and represent the information in a new coordinate system with axes aligned to the directions of maximum variability [@joshi2020prediction]. Most of the data lies in this new feature subspace, where variance is used to measure the amount of information it contains. The quality of this subspace is assessed by comparing its variance to the total variance of the entire dataset, helping PCA identify the main structure in the data [@marukatat2023tutorial].  

PCA is often used during exploratory data analysis because this technique enables graphical visualization of the dataset. This can reveal unexpected relationships between the original variables that would otherwise be challenging to identify [@johnson2023applied]. By reducing the dimensionality, PCA simplifies the data structure, making it easier to interpret and analyze. As a result, trends, patterns, and outliers can be identified in the new reduced-dimension dataset [@richardson2009pca].  

Additionally, PCA is a powerful tool to address the curse of dimensionality and the main problem areas associated with high dimensionality: data sparsity, multicollinearity, and overfitting [@altman2018curse]. By projecting the data onto the principal components, the density of data points is increased in the new vector space which makes it easier to detect patterns and reduce noise. Multicollinearity is mitigated in the process because the principal components are uncorrelated to each other which improves stability and performance of the predictive models [@bharadiya2023tutorial]. With dimensionality reduction, models become simpler which improves generalization and reduces overfitting.  

These benefits become valuable in many real-world applications, including image compression and signal processing. Lengthy image transfer over the internet can be improved by factoring image pixel matrices to extract the main patterns with minimal information loss [@ng2017pca]. For image processing tasks and particularly in edge-computing applications, PCA can file size by 60% with minimal information loss [@ali2024dimensionality]. This is especially useful for real-time quality control systems with deployed ML models in advanced manufacturing environments. For signal processing, PCA can be used to identify the underlying structure and relevant features in signals amongst the noise.  

In machine learning pipelines, PCA is often employed because reducing the number of dimensions decreases computational complexity, lowers memory requirements, and enhances algorithm efficiency. Additionally, PCA's feature extraction capabilities allow for a better understanding and interpretation of the underlying data structure by identifying the most influential variables [@bharadiya2023tutorial]. It effectively filters out noise and irrelevant variations, enhancing the signal-to-noise ratio and improving the performance of subsequent analyses. Using PCA in both the training and deployment stages ensures consistent data transformation, maintains model stability, and boosts prediction speed by working with fewer, more informative features.  

As with every technique, PCA has a few inherent limitations. It assumes linearity which means that if nonlinear relationships exist, PCA will not as effective because it may fail to capture the underlying structure in the data. However, modern variations such as Kernal PCA seeks to address this [@marukatat2023tutorial]. Interpretability is another considerable downside because the resulting principal components are combinations of the original variables. PCA is also sensitive to outliers, which can distort the variance-covariance structure and affect the quality of dimensionality reduction. However, there have been improvements to address outlier sensitivity such as Robust PCA which decomposes the data into a low-rank matrix and sparse matrix to separate signal and noise [@bharadiya2023tutorial].  

Additionally, PCA results in some degree of information loss, as the lower-dimensional representation will not retain all details from the original data. More specifically, in image compression, PCA can lead to loss of fine details and subtle features in images, especially when a high compression rate is applied. In addition, the computation of eigenvectors and eigenvalues, especially for large images, can be resource intensive and time-consuming [@do2012pca]. Lastly, selecting the optimal number of components to retain is subjective; choosing too few can lead to significant information loss, while choosing too many can lead to overfitting or unnecessary complexity.  

Despite these drawbacks, PCA is crucial in data science for its ability to simplify complex datasets, reduce computational costs, and enhance the interpretability of data by focusing on the most significant features. It offers a powerful tool for improving model performance, reducing noise, and uncovering hidden patterns, making it indispensable for efficient and insightful data analysis.





## Methods  

### Linear Algebra Foundations  

Principal Component Analysis (PCA) simplifies multivariate data by transforming the original potentially correlated variables into a smaller set of uncorrelated variables called principal components. The aim of PCA is to reduce the dataset's dimensionality while preserving as much variability as possible. Because the variables will often have different scales or units, standardization is crucial to ensure each variable contributes equally to the analysis. Principal components are obtained by identifying the directions of maximum variability in the data. These components are orthogonal and uncorrelated, with each one capturing a specific amount of the data's variance. This process creates a new coordinate system that simplifies the data structure while retaining its essential characteristics [@johnson2023applied].

While different methods can be used to determine the principal components, Singular Value Decomposition (SVD) is the most common due to its computational efficiency and numerical stability which makes it a popular choice for popular programming packages. SVD decomposes the data into three simpler matrices, making it possible to handle large datasets effectively. This decomposition allows for a more straightforward calculation of the principal components without explicitly computing the covariance matrix, which can be computationally expensive and numerically unstable. The ability to handle sparse and dense matrices further enhances its versatility and efficiency in diverse applications. These advantages make SVD a preferred method in popular programming packages, ensuring that PCA is performed quickly and accurately.

### Overview of Algorithm with SVD:  

1. **Standardize the data**  
   Ensure each variable contributes equally by having a mean of zero and variance of one. This requires that the variables are continuous.
   $$
   X_{\text{standardized}} = \frac{X - \mu}{\sigma}
   $$
   $\mu$: mean, $\sigma$: standard deviation of each variable

2. **Perform Singular Value Decomposition (SVD)**  
  Decompose the standardized data matrix into three matrices $U$, $\Sigma$, and $V^T$.
  $$
  X_{\text{standardized}} = U \Sigma V^T
  $$
  $ùëà$ and $V^T$ are orthogonal matrices, so therefore $U$ and $V$ are also orthogonal. The diagonal matrix of singular values is represented by $\sigma$ with values $\sigma_i$ that are naturally sorted in descending order. Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other in the transformed feature space. The number of PC's initially generated are equivalent to the number of variables initially provided in the dataset.

3. **Selection of principal components**  
  The explained variance of each PC is represented by:
  $$
  \text{variance\_explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
  $$
  For each singular value $\sigma_i$ in the matrix of singular values $\Sigma$, compute $\sigma_i^2$ and calculate the individual variance. The cumulative explained variance target is specified (typically 95%) as a criteria for PC selection. Determine the number of components needs to reach the desired cumulative explained variance.

4. **Transform the data**  
  Project the standardized data onto the selected principal components.
  $$
  X_{\text{transformed}} = X_{\text{standardized}} V_{\text{selected}}
  $$
  where $V_{\text{selected}}$ contains the first number of components columns of $V$. Once the data is transformed onto the selected PC's, it is effectively reduced in dimensionality while retaining most of its original variability.




### Assumptions and Practical Testing  

This section covers the primary assumptions of PCA and details how they can be tested in real-world applications. A more accurate name for this section might be *requirements* since the effectiveness of PCA relies on satisfying these points.

#### 1. Linearity
**Assumption**: PCA assumes that resulting principal components are linear combinations of the original variables. Nonlinear relationships may lead to low covariance values which can lead to an undervalued representation of their signficance.

**Testing**: Initial checks with scatter plots and correlation matrices can help identify linear relationships between variable pairs. While common, these methods require manual inspection which introduces the potential for the researcher to miss subtler non-linear patterns. Supplementing with statistical tests for linearity can provide a more robust assessment. Transformations can be applied to specific non-linear variables or alternative PCA methods can be utilized such as Kernal PCA [@marukatat2023tutorial].


##### 2. Continuous data
**Assumption**: To calculate principal components, data should be on a continuous scale: either interval or ratio. It should be mentioned that despite being ordinal, Likert Scales are often used because the distance between scale points are assumed to be approximately equivalent.  

**Testing**: Reviewing column data types and value counts are practical methods for testing. Simple transformations can be applied to categorical data.

#### 3. Data standardization 

**Assumption**: There are three critical preprocessing steps: scaling, mean-centering, and outlier handling. Scaling standardizes the variance of each variable to ensure equal contributions. Mean-centering has a similar impact: ensuring that the principal components capture the true direction of maximum variance. Outliers can also distort the principal components, so they should be identified and handled appropriately. Together these methods prevent variables with larger measured values from dominating the principal components and skewing results.

**Testing**: Data will usually not arrive in a condition that meets this requirement. Luckily, statistical packages in common programming languages offer simple methods to scale and mean-center data. Outliers can be detected using a variety of distribution plots, but automated methods could be employed for larger datasets which is beyond the scope of this paper.  

! need to add Varimax, and what loadings are 


## Application 1 - Analysis and Results

Load the required libaries:
```{r}
library(readxl)
library(skimr)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(corrplot)
library(DescTools)
library(reshape2)
library(factoextra)
library(kableExtra)
library(rrcov)
library(psych)
library(stats)
library(ggfortify)
```

### Dataset Description

The dataset contains demographic, health, and environmental metrics for counties in the United States. Each row represents data for a specific county of a state. The columns analyzed were:  
- obesity_age_adj: Age-adjusted obesity rate.  
- Smoking_Rate: Rate of smoking within the population.  
- Diabetes: Diabetes prevalence rate.  
- Heart_Disease: Heart disease prevalence rate.  
- Cancer: Cancer prevalence rate.  
- Food Index: Index score representing food availability and quality.  
- Poverty_Percent: Percentage of the population living below the poverty line.  
- physical_inactivity: Rate of physical inactivity.  
- Mercury_TPY: Mercury emissions in tons per year.  
- Lead_TPY: Lead emissions in tons per year.  
- Atrazine_High_KG: Atrazine in KG per year.  

```{r}
url <- "https://raw.githubusercontent.com/nickbelgau/STA6257_PCA/main/data/Alz_mortality_data_complete_with_state_csv.csv"
data_raw <- read_csv(url)
```

### Filtered Dataset and Inspection  

The dataset was filtered to analyze only selected variables for states in the deep south in the United States. The county and state columns were removed, creating 1 entire region on the dataset. Columns were selected to remove demographic fields related to gender, sex, and race and ambigious fields related to grouping.

```{r}
selected_columns <- c(
  "County", "State", "obesity_age_adj", "Smoking_Rate", "Diabetes", "Heart_Disease", "Cancer",  "Mercury_TPY", "Lead_TPY", "Food_index", "Poverty_Percent", "Atrazine_High_KG", "SUNLIGHT"
)

deep_south_states <- c("AL", "AR", "FL", "GA", "LA", "MS", "NC", "SC", "TN", "TX", "VA")

data <- data_raw %>%
  filter(State %in% deep_south_states) %>%
  select(all_of(selected_columns))

data <- data %>%
  select(-County, -State)
```

The variables appear to be continuous because the data types are "numeric" with high cardinality. There are no nulls. It is clear that scaling and mean-centering will be needed.  

```{r}
skim(data)
```

### Linarity Analysis  
PCA works on the premise that principal components are linear combinations of the original features. Linearity between variables can enhance the effectiveness of PCA, and lack thereof leads to information loss. The Harvey-Collier Test for Linearity can be used to conduct pairwise tests to gain more insight into the data. A test like this can be automated to flag variables. This is helpful for large datasets where visually inspecting each relationship can be time-consuming and ineffective. 

The Harvey-Collier Test that fits a linear model, calculates the residuals, and runs regression on those residuals. If the slope of the line-of-best-fit for the residuals is nonzero, the test may fail the linearity check. The test fails because the p-value is less than the significance level, so the relationship is nonlinear.

[RUN TEST ON ALL VARIABLES OF DATASET]
[SHOW TABULAR P-VALUE RESULTS OR ON HEAT MAP (COLOR CODE BELOW THRESHOLD)]
A significane level of \alpha = 0.05 was used.
```{r}
library(car)
library(ggplot2)
library(reshape2)

# Function to run Harvey-Collier Test
harvey_collier_test <- function(data, x, y) {
  model <- lm(as.formula(paste(y, "~", x)), data = data)
  tryCatch({
    test <- car::crPlots(model, variable = x, id.n = 0)
    return(test$p)
  }, error = function(e) {
    return(NA)  # Return NA if the test fails
  })
}

# Calculate p-values
# variable_names <- names(data)
# p_values <- matrix(NA, nrow = length(variable_names), ncol = length(variable_names), 
#                    dimnames = list(variable_names, variable_names))

# for (i in 1:length(variable_names)) {
#   for (j in 1:length(variable_names)) {
#     if (i != j) {
#       p_values[i, j] <- harvey_collier_test(data, variable_names[i], variable_names[j])
#     }
#   }
# }
# print(p_values)

# # Plot the p-values
# p_values_df <- melt(as.data.frame(p_values), varnames = c("Variable1", "Variable2"), value.name = "P_Value")

# ggplot(p_values_df, aes(x = Variable1, y = Variable2, fill = P_Value)) +
#   geom_tile() +
#   scale_fill_gradient2(low = "white", high = "white", mid = "white", midpoint = 0.05,
#                        na.value = "#215B9D", limits = c(0, 0.05), oob = scales::squish) +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
#   labs(title = "Heatmap of P-values from Harvey-Collier Test", fill = "P-Value")


```


[HIGHLIGHT 1 RELATIONSHIP]  
'obesity_age_adj'  
X = sm.add_constant(X)  
'Diabetes'  

The residual plot between these variables confirms the nonlinear relationship which is why the test failed. Sometimes transformations can be helpful, but the risk is nonlinearizing the other relationships between the transformed variable and the rest of the dataset. While Linearity is crucial for optimal performance of PCA, its absence does not render this method unusable. No transformations were conducted and there is acknowledgement that PCA will result in some information loss.


### Outliers analysis  
Outliers can distort PCA results by disproportionately increasing variance, shifting the direction of principal components, and inflating eigenvalues. Before implementing outlier removal techniques, it is crucial to first examine the distribution. This helps ensure such methods are appropriately applied and to validate the rationale behind outlier exclusion.  

While PCA does not require normality, data that is roughly normally distributed can help minimize the impact of outliers. If data is found to be non-normal, transformations can be conducted to create a bell-shape curve that is approximately normal. Evaluating skewness and kurtosis provides a more practical approach for assessing distribution characteristics.

```{r}
library(moments)

distribution_metrics <- function(df) {
  results <- data.frame(
    Skewness = sapply(data, skewness),
    Kurtosis = sapply(data, kurtosis)
  )
  results <- results[order(-results$Kurtosis),]
  return(results)
}

print(distribution_metrics(data))
```
This analysis reveals that Atrazine_High_KG, Mercury_TPY, and Lead_TPY warrant closer inspection. A quick scan of the boxplots confirm that these are variables of interest that should be transformed to improve PCA. These variables are right-skewed and will have a significant amount of outliers if unadjusted.
```{r}
columns_to_transform <- c("Lead_TPY", "Mercury_TPY", "Atrazine_High_KG")

par(mfrow=c(3, 1)) # format
for (col in columns_to_transform) {
  boxplot(
    data[[col]],
    horizontal=TRUE,
    main=paste("Boxplot of", col),
    col="lightblue",
    border="darkblue"
  )
}
```

The Box-Cox transformation is a robust statistical method that normalizes the shape of data, enhancing its suitability for PCA. It is considered best practice to apply such transformations to columns selectively rather than on the entirety of the dataset, even when using Box-Cox. Automating this process can be achieved by setting a kurtosis threshold to run a Box-Cox on. Additionally, negative values must be carefully managed to ensure the Box-Cox transformation is correctly implemented. 
```{r}
library(MASS)

box_cox_transform <- function(df, columns) {
  transformed_df <- df
  lambdas <- list()
  for (col in columns) {
    col_data <- df[[col]]
    col_data[col_data <= 0] <- min(col_data[col_data > 0]) / 2
    bc <- boxcox(col_data ~ 1, plotit=FALSE)
    lambda <- bc$x[which.max(bc$y)]
    transformed_df[[col]] <- (col_data^lambda - 1) / lambda
    lambdas[[col]] <- lambda
  }
  return(list(transformed_df, lambdas))
}

result <- box_cox_transform(data, columns_to_transform)
data_transform <- result[[1]]
lambdas <- result[[2]]

par(mfrow = c(length(columns_to_transform) + 1, 1), mar = c(4, 4, 2, 2))
for (i in columns_to_transform) {
  hist(data_transform[[i]], probability = TRUE, main = paste("Density Plot of", i), xlab = "Values", col = "lightblue", border = "darkblue")
  lines(density(data_transform[[i]]), col = "darkred", lwd = 2)
}
```
The Box-Cox transformations successfully yielded an approximate bell-shaped distribution for these variables, effectively mitigating the influence of outliers. The executed tranformations were minor power adjustments given by the lambda values. This ensure PCA will be more effective without necessitating the manual removal individual data points.  
```{r}
lambda_df <- data.frame(
  Variable = names(lambdas),
  Lambda = unlist(lambdas)
)
print(lambda_df)
```

### Correlation Analysis
Correlation, multicolinearity, information, 

Heatmap
```{r}
library(corrplot)

cor_matrix <- cor(data, use = "complete.obs")  # Handle missing values

color_palette <- colorRampPalette(c("#215B9D", "#DCE6F1", "#215B9D"))(200) # blue

corrplot(abs(cor_matrix), method = "color",
        #  type = "lower", 
         order = "hclust",
         addCoef.col = "#36454F",
         number.cex = 0.50,
         tl.col = "black",
         tl.srt = 45,  # No rotation for text labels
        #  tl.pos = "d",  # Position text labels at the bottom (x-axis)
         cl.pos="n",
         col = color_palette,
         bg = "white"
)
```

### Run Principal Component Analysis

The prcomp() function allows parameters to scale and mean-center at the time of executing PCA.
```{r}
pca_result <- prcomp(data_transform, center=TRUE, scale.=TRUE)
pca_summary <- summary(pca_result)
importance <- as.data.frame(pca_summary$importance)
importance <- t(importance) # transpose to make cleaner
colnames(importance) <- c("Std Dev", "Proportion", "Cumulative Var")
importance
```
The target of 95% cumulative variance is maintained with the first 9 principal components, indicating a minor improvement in dimensionality reduction. Ideally, closer to 95% variance with fewer dimensions would be preferable, but this was not achieved in this case. It is still valuable to implement PCA despite the minor improvement.  

These results demonstrate an improvement in the explained variance compared to the untransformed dataset:  
[INSTEAD OF TABULAR, SHOW AS STACKED LINE CHART OF CUMULATIVE VARIANCE BY PC, highlight the delta]  
```{r}
pca_result <- prcomp(data, center=TRUE, scale.=TRUE)
pca_summary <- summary(pca_result)
importance <- as.data.frame(pca_summary$importance)
importance <- t(importance) # transpose to make cleaner
colnames(importance) <- c("Std Dev", "Proportion", "Cumulative Var")
importance
```

View the PCA components (loadings)
```{r}
# eigenvalues <- (pca_result$sdev)^2 / (nrow(data) - 1)

# eigenvectors <- pca_result$rotation

# eigenvalues
# eigenvectors
```


Screeplot
```{r}
plot(pca_result, type = "l", col = "#215B9D", lwd = 2)
```
Elbow method


Biplot  
Instead of printing the loadings in a tabular format, it can be helpful to leverage a biplot to visually reveal the individual contributions. The points on the graph are the original data points projected on the first two principal component axes. The vectors (or arrows) are the contribution of each of the original variables to the principal components. 

Arrow direction: Variables that are orientated in the same direction or in the complete opposite direction are correlated in the positive or negative direction, respectively. This indicates a redunacy in information which PCA can help reduce.  
Arrow magnitude: The length of the arrow signals the magnitude in which the original variable contributes to the PC. 
```{r}
autoplot(pca_result, 
         data = data,
         colour = 'grey',
         loadings = TRUE,
         loadings.colour = '#215B9D',
         loadings.label = TRUE,
         loadings.label.colour = 'black', 
         loadings.label.size = 3) + 
  theme_minimal() +
  theme(legend.position = "none"
)
```
From the results, it's clear that Poverty_Percent and Food_index are negatively correlated and contribute redunant information to the dataset. Additional groups can be found such as obesity_age_adj and Diabetes which are positively correlated; Heart_Disease, Smoking_Rate, and Cancer; and so on. The biplot provides valuable information to understand these relationships.

Multicollinearity
PCA aids in building more robust statistical models by reducing multicollinearity by eliminating redunant information. If we were to model this dataset and arbitrarily choose a y-variable such as Heart_Disease, we can inspect the collinearity prior to PCA.

Compare VIF (variance inflation factor) before PCA:
```{r}
library(car)
data_df <-data.frame(data)
model <- lm(Heart_Disease ~ ., data=data_df)

data.frame(VIF=vif(model))
```
Just as the biplot indicated, multicollinearity exists between obesity_age_adj and Diabetes. After implementing PCA, the principal components are orthogonal to each other and have no correlation. Reducing multicollinearity improves the model's stability, interpretability, and generalizability by simplifying it.

## Application 2: Image Compression  

New example.
No scree plot or biplot because pixels.
Results to focus on memory reduction and maintaining equivalent level of performance.
This will be similar to appl 1 + 2 but without going into as much detail for techniques already demonstrated.

## Conclusion  

Summary, reflections, future discissions

