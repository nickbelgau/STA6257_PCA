---
title: "Practical Implementations of Principal Component Analysis"
author: "Nick Belgau, Oscar Hernandez Mata"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
jupyter: 
  kernel: "python3"
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Across diverse industries and data science applications, Principal Component Analysis (PCA) offers a powerful solution for extracting valuable insights from complex datasets. The primary objective of PCA is to reduce the number of dimensions in the dataset while retaining most of the original information. This is directly applicable to big data scenarios where it is not feasible to rely on univariate analysis to find patterns, and redundancy in the data leads to inflated standard errors and poor model stability [@rahayu2017application].  

Developed by Karl Pearson in 1901, PCA utilizes linear algebra to transform the original dataset to a lower dimensional vector space of uncorrelated variables known as principal components. These are linear combinations of the original variables, and represent the information in a new coordinate system with axes aligned to the directions of maximum variability [@joshi2020prediction]. Most of the data lies in this new feature subspace, where variance is used to measure the amount of information it contains. The quality of this subspace is assessed by comparing its variance to the total variance of the entire dataset, helping PCA identify the main structure in the data [@marukatat2023tutorial].  

PCA is often used during exploratory data analysis because this technique enables graphical visualization of the dataset. This can reveal unexpected relationships between the original variables that would otherwise be challenging to identify [@johnson2023applied]. By reducing the dimensionality, PCA simplifies the data structure, making it easier to interpret and analyze. As a result, trends, patterns, and outliers can be identified in the new reduced-dimension dataset [@richardson2009pca].  

Additionally, PCA is a powerful tool to address the curse of dimensionality and the main problem areas associated with high dimensionality: data sparsity, multicollinearity, and overfitting [@altman2018curse]. By projecting the data onto the principal components, the density of data points is increased in the new vector space which makes it easier to detect patterns and reduce noise. Multicollinearity is mitigated in the process because the principal components are uncorrelated to each other which improves stability and performance of the predictive models [@bharadiya2023tutorial]. With dimensionality reduction, models become simpler which improves generalization and reduces overfitting.  

These benefits become valuable in many real-world applications, including image compression and signal processing. Lengthy image transfer over the internet can be improved by factoring image pixel matrices to extract the main patterns with minimal information loss [@ng2017pca]. For image processing tasks and particularly in edge-computing applications, PCA can reduce file size by 60% with minimal information loss [@ali2024dimensionality]. This is especially useful for real-time quality control systems with deployed ML models in advanced manufacturing environments. For signal processing, PCA can be used to identify the underlying structure and relevant features in signals amongst the noise.  

In machine learning pipelines, PCA is often employed because reducing the number of dimensions decreases computational complexity, lowers memory requirements, and enhances algorithm efficiency. Additionally, PCA's feature extraction capabilities allow for a better understanding and interpretation of the underlying data structure by identifying the most influential variables [@bharadiya2023tutorial]. It effectively filters out noise and irrelevant variations, enhancing the signal-to-noise ratio and improving the performance of subsequent analyses.

As with every technique, PCA has a few inherent limitations. It assumes linearity which means that if nonlinear relationships exist, PCA will not as effective because it may fail to capture the underlying structure in the data. However, modern variations such as Kernal PCA seeks to address this [@marukatat2023tutorial]. Interpretability is another considerable downside because the resulting principal components are combinations of the original variables, but interpretting the loadings can be helpful in this effort. PCA is also sensitive to outliers, which can affect the quality of dimensionality reduction. However, there have been improvements to address outlier sensitivity such as Robust PCA which decomposes the data into a low-rank matrix and sparse matrix to separate signal and noise [@bharadiya2023tutorial].  

Additionally, PCA results in some degree of information loss, as the lower-dimensional representation will not retain all details from the original data. More specifically, in image compression, PCA can lead to loss of fine details and subtle features in images, especially when a high compression rate is applied. In addition, the computation of eigenvectors and eigenvalues, especially for large images, can be resource intensive and time-consuming [@do2012pca]. Lastly, selecting the optimal number of components to retain is subjective; choosing too few can lead to significant information loss, while choosing too many can lead to overfitting or unnecessary complexity.  

Despite these drawbacks, PCA is crucial in data science for its ability to simplify complex datasets, reduce computational costs, and enhance the interpretability of data by focusing on the most significant features. It offers a powerful tool for improving model performance, reducing noise, and uncovering hidden patterns, making it indispensable for efficient and insightful data analysis.


## Methods  

### Linear Algebra Foundations  

Principal Component Analysis (PCA) simplifies multivariate data by transforming the original potentially correlated variables into a smaller set of uncorrelated variables called principal components. The aim of PCA is to reduce the dataset's dimensionality while preserving as much variability as possible. Because the variables will often have different scales or units, standardization is crucial to ensure each variable contributes equally to the analysis. Principal components are obtained by identifying the directions of maximum variability in the data. These components are orthogonal and uncorrelated, with each one capturing a specific amount of the data's variance. This process creates a new coordinate system that simplifies the data structure while retaining its essential characteristics [@johnson2023applied].

While different methods can be used to determine the principal components, Singular Value Decomposition (SVD) is the most common due to its computational efficiency and numerical stability which makes it a popular choice for popular programming packages. SVD decomposes the data into three simpler matrices, making it possible to handle large datasets effectively. This decomposition allows for a more straightforward calculation of the principal components without explicitly computing the covariance matrix, which can be computationally expensive and numerically unstable. The ability to handle sparse and dense matrices further enhances its versatility and efficiency in diverse applications. These advantages make SVD a preferred method in popular programming packages, ensuring that PCA is performed quickly and accurately.

### Overview of Algorithm with SVD:  

1. **Standardize the data**  
   Ensure each variable contributes equally by having a mean of zero and variance of one. This requires that the variables are continuous.
   $$
   X_{\text{standardized}} = \frac{X - \mu}{\sigma}
   $$
   $\mu$: mean, $\sigma$: standard deviation of each variable

2. **Perform Singular Value Decomposition (SVD)**  
  Decompose the standardized data matrix into three matrices $U$, $\Sigma$, and $V^T$.
  $$
  X_{\text{standardized}} = U \Sigma V^T
  $$
  $ùëà$ and $V^T$ are orthogonal matrices, so therefore $U$ and $V$ are also orthogonal. The diagonal matrix of singular values is represented by $\sigma$ with values $\sigma_i$ that are naturally sorted in descending order. Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other in the transformed feature space. The number of PC's initially generated are equivalent to the number of variables initially provided in the dataset.

3. **Selection of principal components**  
  The explained variance of each PC is represented by:
  $$
  \text{variance\_explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
  $$
  For each singular value $\sigma_i$ in the matrix of singular values $\Sigma$, compute $\sigma_i^2$ and calculate the individual variance. The cumulative explained variance target is specified (typically 95%) as a criteria for PC selection. Determine the number of components needs to reach the desired cumulative explained variance.

4. **Transform the data**  
  Project the data onto the selected principal components. This yields the new subspace with reduced dimensions.
  $$
  X_{\text{transformed}} = X_{\text{standardized}} V_{\text{selected}}
  $$
  where $V_{\text{selected}}$ contains the first number of components columns of $V$. Once the data is transformed onto the selected PC's, it is effectively reduced in dimensionality while retaining most of its original variability.




### Assumptions and Practical Testing  

This section covers the primary assumptions of PCA and details how they can be tested in real-world applications. A more accurate name for this section might be *requirements* since the effectiveness of PCA relies on satisfying these points.

#### 1. Linearity
**Assumption**: PCA assumes that resulting principal components are linear combinations of the original variables. Nonlinear relationships may lead to low covariance values which can lead to an undervalued representation of their signficance.

**Testing**: Initial checks with scatter plots and correlation matrices can help identify linear relationships between variable pairs. While common, these methods require manual inspection which introduces the potential for the researcher to miss subtler non-linear patterns. Supplementing with statistical tests for linearity can provide a more robust assessment. Transformations can be applied to specific non-linear variables or alternative PCA methods can be utilized such as Kernal PCA [@marukatat2023tutorial].


##### 2. Continuous data
**Assumption**: To calculate principal components, data should be on a continuous scale: either interval or ratio. It should be mentioned that despite being ordinal, Likert Scales are often used because the distance between scale points are assumed to be approximately equivalent.  

**Testing**: Reviewing column data types and value counts are practical methods for testing. Simple transformations can be applied to categorical data.

#### 3. Data standardization 

**Assumption**: There are three critical preprocessing steps: scaling, mean-centering, and outlier handling. Scaling standardizes the variance of each variable to ensure equal contributions. Mean-centering has a similar impact: ensuring that the principal components capture the true direction of maximum variance. Outliers can also distort the principal components, so they should be identified and handled appropriately. Together these methods prevent variables with larger measured values from dominating the principal components and skewing results.

**Testing**: Data will usually not arrive in a condition that meets this requirement. Luckily, statistical packages in common programming languages offer simple methods to scale and mean-center data. Outliers can be detected using a variety of distribution plots, but automated methods could be employed for larger datasets which is beyond the scope of this paper.  


## Application 1 - Analysis and Results

Load the required libaries:
```{r}
library(readxl)
library(skimr)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(corrplot)
library(DescTools)
library(reshape2)
library(factoextra)
library(kableExtra)
library(rrcov)
library(psych)
library(stats)
library(ggfortify)
library(lmtest)
library(car)
```

### Dataset Description

The dataset contains demographic, health, and environmental metrics for counties in the United States from US census data. Each row represents data for a specific county of a state [@tejada_vera_2013] [@amin_2018]. The columns analyzed were:  
- obesity_age_adj: Age-adjusted obesity rate.  
- Smoking_Rate: Rate of smoking within the population.  
- Diabetes: Diabetes prevalence rate.  
- Heart_Disease: Heart disease prevalence rate.  
- Cancer: Cancer prevalence rate.  
- Food Index: Index score representing food availability and quality.  
- Poverty_Percent: Percentage of the population living below the poverty line.  
- physical_inactivity: Rate of physical inactivity.  
- Mercury_TPY: Mercury emissions in tons per year.  
- Lead_TPY: Lead emissions in tons per year.  
- Atrazine_High_KG: Atrazine in KG per year.  

```{r}
url <- "https://raw.githubusercontent.com/nickbelgau/STA6257_PCA/main/data/demographic/Alz_mortality_data_complete_with_state_csv.csv"
data_raw <- read_csv(url)
```

### Filtered Dataset and Inspection  

The dataset was filtered to analyze only selected variables for states in the deep south in the United States. The county and state columns were removed, creating 1 entire region on the dataset. Columns were selected to remove demographic fields related to gender, sex, and race and ambigious fields related to grouping.

```{r}
selected_columns <- c(
  "County", "State", "obesity_age_adj", "Smoking_Rate", "Diabetes", "Heart_Disease", "Cancer",  "Mercury_TPY", "Lead_TPY", "Food_index", "Poverty_Percent", "Atrazine_High_KG", "SUNLIGHT"
)

deep_south_states <- c("AL", "AR", "FL", "GA", "LA", "MS", "NC", "SC", "TN", "TX", "VA")

data <- data_raw %>%
  filter(State %in% deep_south_states) %>%
  select(all_of(selected_columns))

data <- data %>%
  select(-County, -State)
```

The variables appear to be continuous because the data types are "numeric" with high cardinality. There are no nulls. It is clear that scaling and mean-centering will be needed.  

```{r}
skim(data)
```

### Linarity Analysis  
PCA works on the premise that principal components are linear combinations of the original features. Linearity between variables boosts PCA efficiency, while its absence may cause information loss. Statistical pairwise testing can automate the identification of non-linear relationships, providing a practical alternative to time-intensive visual inspection methods which is particularly helpful for large datasets.  

The Harvey-Collier Test for Linearity fits a linear model to each variable pair, calculates recursive residuals, and conducts regression again on these residuals [@nwakuya2022instability] [@harvey1977testing]. A significant nonzero slope in the residual regression indicates non-linearity, suggesting potential information loss in PCA.

```{r}
harvey_collier_test <- function(data, x, y) {
  formula <- as.formula(paste(y, "~", x))
  model <- lm(formula, data = data)
  test <- harvtest(model)
  p_value <- test$p.value
  return(signif(p_value, digits = 2))
}

variables <- names(data)
n <- length(variables)
p_matrix <- matrix(NA, n, n, dimnames = list(variables, variables))

for (i in 1:n) {
  for (j in 1:n) {
    if (i != j) {  # Avoid testing a variable against itself
      p_matrix[i, j] <- harvey_collier_test(data, variables[i], variables[j])
    }
  }
}
```
When the p-value is less than the signficance level, the test declares that the relationship is nonlinear. A heatmap of the p-values reveals which pairs are nonlinear with color-coding. 
```{r}
library(ggplot2)

p_matrix_long <- melt(p_matrix)
names(p_matrix_long) <- c("Var1", "Var2", "p_value")

p_matrix_long$Var1 <- factor(p_matrix_long$Var1, levels = rev(unique(p_matrix_long$Var1)))

alpha = 0.0001

gradient_fill <- scale_fill_gradientn(
  colors = c("#215B9D", "#F0F0F0", "#F0F0F0"),
  values = scales::rescale(c(0, alpha, 1)),
  na.value = "#F0F0F0",  # Also set missing values to light grey
  guide = "colourbar"
)

ggplot(p_matrix_long, aes(Var1, Var2, fill= p_value)) + 
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2e", p_value)), color = "black", size = 2) + 
  gradient_fill +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") + 
  labs(title = "Heatmap of P-Values") 
```

The residual plot for a single pair that was flagged as nonlinear reveals why the Harvey-Collier Test declared nonlinearity. Although transformations can adjust linearity, they risk altering other variable relationships. Thus, no transformations were applied, acknowledging some information loss in PCA.

```{r}
data_residual <- as.data.frame(data)

data_residual$residuals <- residuals(lm(Diabetes ~ obesity_age_adj, data = data_residual))

ggplot(data_residual, aes(x = obesity_age_adj, y = residuals)) +
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE, color = "blue") +  # LOWESS curve
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals of Diabetes vs. obesity_age_adj",
       x = "obesity_age_adj", y = "Residuals") +
  theme_minimal()
```


### Outliers Analysis  
Outliers can distort PCA results by disproportionately increasing variance, shifting the direction of principal components, and inflating eigenvalues. Before implementing outlier removal techniques, it is crucial to first examine the distribution to validate the rationale behind outlier exclusion. Although PCA does not require normality, a roughly normal distribution minimizes the impact from outliers. Non-normal data may undergo transformations like the Box-Cox to approximate normality. Assessing skewness and kurtosis offers practical insights into distribution characteristics.

```{r}
library(moments)

distribution_metrics <- function(df) {
  results <- data.frame(
    Kurtosis = sapply(data, kurtosis),
    Skewness = sapply(data, skewness)
  )
  results <- results[order(-results$Kurtosis),]
  return(results)
}

print(distribution_metrics(data))
```
This analysis reveals columns with high kurtosis and skewness. Boxplots confirm that these are variables are right-skewed with numerous outliers, suggesting the need for transformation to improve PCA.
```{r}
columns_to_transform <- c("Lead_TPY", "Mercury_TPY", "Atrazine_High_KG")

par(mfrow=c(3, 1)) # format
for (col in columns_to_transform) {
  boxplot(
    data[[col]],
    horizontal=TRUE,
    main=paste("Boxplot of", col),
    col="lightblue",
    border="darkblue"
  )
}
```

The Box-Cox transformation is a robust statistical method that normalizes the data distribution, enhancing its suitability for PCA [@compression2014boxcox]. It is considered best practice to apply such transformations to columns selectively rather than on the entirety of the dataset. Automating this process can be achieved by setting a kurtosis threshold.Additionally, negative values must be carefully managed to ensure the Box-Cox transformation is correctly implemented. 
```{r}
library(MASS)

box_cox_transform <- function(df, columns) {
  transformed_df <- df
  lambdas <- list()
  for (col in columns) {
    col_data <- df[[col]]
    col_data[col_data <= 0] <- min(col_data[col_data > 0]) / 2
    bc <- boxcox(col_data ~ 1, plotit=FALSE)
    lambda <- bc$x[which.max(bc$y)]
    transformed_df[[col]] <- (col_data^lambda - 1) / lambda
    lambdas[[col]] <- lambda
  }
  return(list(transformed_df, lambdas))
}

result <- box_cox_transform(data, columns_to_transform)
data_transform <- result[[1]]
lambdas <- result[[2]]

par(mfrow = c(length(columns_to_transform) + 1, 1), mar = c(4, 4, 2, 2))
for (i in columns_to_transform) {
  hist(data_transform[[i]], probability = TRUE, main = paste("Density Plot of", i), xlab = "Values", col = "lightblue", border = "darkblue")
  lines(density(data_transform[[i]]), col = "darkred", lwd = 2)
}
```
The transformations create an approximate bell-shaped distribution, reducing outlier effects. Minor power adjustments, indicated by lambda values, enhance PCA effectiveness without the need for manual outlier removal.  
```{r}
lambda_df <- data.frame(
  Lambda = unlist(lambdas)
)
print(lambda_df)
```

### Correlation Analysis
A heatmap of the correlation matrix was analyzed to identify highly correlated variables, indicating potential multicollinearity within the dataset. By applying PCA, correlated variables can be transformed into orthogonal components which eliminate multicollinearity. This transformation ensures each principal component captures unique variance, improving the clarity and reliability of modeling.

```{r}
library(corrplot)

cor_matrix <- cor(data, use = "complete.obs")  # Handle missing values

color_palette <- colorRampPalette(c("#215B9D", "#DCE6F1", "#215B9D"))(200) # blue #215B9D

corrplot(abs(cor_matrix), method = "color",
        #  type = "lower", 
         order = "hclust",
         addCoef.col = "#36454F",
         number.cex = 0.50,
         tl.col = "black",
         tl.srt = 45,  # No rotation for text labels
        #  tl.pos = "d",  # Position text labels at the bottom (x-axis)
         cl.pos="n",
         col = color_palette,
         bg = "white"
)
```

### Principal Component Analysis

The prcomp() function allows parameters to scale and mean-center at the time of executing PCA.
```{r}
pca_result <- prcomp(data_transform, center=TRUE, scale.=TRUE)
pca_summary <- summary(pca_result)
importance <- as.data.frame(pca_summary$importance)
importance <- as.data.frame(t(importance)) # transpose to make cleaner

importance$Eigenvalues <- pca_result$sdev^2
colnames(importance) <- c("Std Dev", "Proportion", "Cumulative Variance", "Eigenvalues")
importance <- importance[, c("Std Dev", "Eigenvalues", "Proportion", "Cumulative Variance")] # rearrangeo
importance
```
In PCA, the target explained variance is decided after acknowleding a trade-off between information retention and dimensionality reduction; common practice is to aim for 70-95%. The first principal component captures a substantial underlying pattern in the dataset, accounting for over 33% of the explained variance. The first few components are crucial for capturing the major variance in the data. Later components appear to represent more refined details in the data, tapering off until finally 95% variance is obtained by the ninth principal component.  

The scree plot shows an elbow at the fourth principal component, indicating a point of diminishing returns. Focusing on the first four components might be optimal depending on the objective. They likely provide a sufficient summary of the data with significant variance coverage while avoiding overfitting. Although it should be noted that for a dataset this small, dimensionality reduction is likely not valued as a priority objective.  
```{r}
plot(pca_result, type = "l", col = "#215B9D", lwd = 2)
```

Eigenvectors, or loadings, represent the weight of each of the original variables in the linear combination to form the new principal components. Loadings close to -1 or 1 means the original variable has a significant contribution. The loadings can be summarized by principal component to reveal interesting  structure to the data.  

**PC1**: Strong positive loadings for obesity_age_adj, Smoking_Rate, Diabetes, Heart_Disease, Cancer, and Poverty_Percent, likely represents general health and lifestyle factors. This component suggests that increases in these variables correlate with poorer health outcomes.     
**PC2**: Strong negative loadings for Mercury_TPY and Lead_TPY, indicating association with environmental exposure. Higher exposure levels will cause the score on this PC to decrease due to the sign of the loading value.  
**PC3**: While SUNLIGHT has the highest loading and positive contribution to this PC score, the relationship between Food_index and Poverty_index is interesting. These two variables are negatively correlated which means as access to higher quality food decreases, poverty level will increase and in turn both of these factors will reduce the score of this principal component.  
**PC4**: This PC is largely influenced by the chemical Atrazine and has some correlation to Cancer. This could point towards a link between agricultural practices and these variables.
```{r}
eigenvectors <- pca_result$rotation
first_four_eigenvectors <- eigenvectors[, 1:4]
first_four_eigenvectors
```

A biplot is an effective visualization tool in PCA that combines the projection of the original data points and the vectors representing each variable's contribution onto the principal component axes. The points on the graph are the original data points projected on the first two principal component axes. The vectors (or arrows) are the contribution of each of the original variables to the principal components. The directions of the principal components are dictated by the loadings. Variables that are orientated in the same direction or in the complete opposite direction are correlated in the positive or negative direction, respectively. This indicates a redundancy in information which PCA can help reduce. The arrow length signals the magnitude in which the original variable contributes to the PC.  
```{r}
autoplot(pca_result, 
         data = data_transform,
         colour = 'grey',
         loadings = TRUE,
         loadings.colour = '#215B9D',
         loadings.label = TRUE,
         loadings.label.colour = 'black', 
         loadings.label.size = 3) + 
  theme_minimal() +
  theme(legend.position = "none"
)
```
From the results, it's clear that Food_index is negatively correlated to obesity_age_adj and Diabetes which contributes redundant information to the dataset. Additional positively correlated groups are found due to the small angle between the vectors: Mercuryy_TPY and Lead_TPY; Heart_Disease, Smoking_Rate, and Poverty; and so on. The biplot provides valuable information to understand these relationships.  


PCA aids in building more robust statistical models by reducing multicollinearity and removing redundant information. For example, consider a scenario where 'Heart_Disease' is chosen as the dependent variable to model. Before applying PCA, the collinearity can be assessed by using the Variance Inflation Factor (VIF). Initial VIF results confirm multicollinearity between variables such as 'obesity_age_adj' and 'Diabetes', as suggested by the biplot. After implementing PCA, the principal components are orthogonal to each other and, therefore, uncorrelated. This reduction in multicollinearity not only stabilizes and simplifies the model but also enhances its interpretability and generalizability [@altman2018curse].
```{r}
library(car)
data_df <-data.frame(data_transform)
model <- lm(Heart_Disease ~ ., data=data_df)

data.frame(VIF=vif(model))
```

Lastly, the impact of transforming certain variables using the Box-Cox transformation resulted in an improvement by retaining more information. This is apparent by the higher explained variance.
```{r}
# conduct PCA on the original untransformed dataset
pca_result_original <- prcomp(data, center=TRUE, scale.=TRUE)
pca_summary_original <- summary(pca_result_original)
importance_original <- as.data.frame(pca_summary_original$importance)
importance_original <- as.data.frame(t(importance_original))
importance_original$Eigenvalues <- pca_result_original$sdev^2
colnames(importance_original) <- c("Std Dev", "Proportion", "Cumulative Variance", "Eigenvalues")
importance_original <- importance_original[, c("Std Dev", "Eigenvalues", "Proportion", "Cumulative Variance")] # rearrange

importance <- as.data.frame(importance)
importance$PC <- rownames(importance)
importance$PC <- as.numeric(substring(importance$PC, 3))
importance$name <- "transform"

importance_original <- as.data.frame(importance_original)
importance_original$PC <- rownames(importance_original)
importance_original$PC <- as.numeric(substring(importance_original$PC, 3))
importance_original$name <- "original"

combined_results <- rbind(importance, importance_original)
rownames(combined_results) <- NULL
combined_results$PC <- factor(combined_results$PC)

ggplot(combined_results, aes(x=PC, y=`Cumulative Variance`, color=name, group=name)) +
  geom_line(linewidth=1.1) +
  labs(title = "Comparison of Cumulative Variance",
       x = "Principal Component",
       y = "Cumulative Variance") +
  theme_minimal() +
  scale_color_manual(values = c("black", "#215B9D")) +
  scale_x_discrete()
```
 
## Application 2: Image Compression  
Don't use PCA for image classification. Use neural networks.[need sources]

```{r}}
library(reticulate)

# Use reticulate to ensure TensorFlow is installed
py_install("tensorflow", pip = TRUE)
```
Data Loading and Normalization:

Data Loading and Transformation
The CIFAR-10 dataset was loaded and normalized to have pixel values between 0 and 1.
Training, validation, and test sets were created with the training set further split to include a validation set.

```{python}
import tensorflow as tf
```

Load the data and normalize
```{python}
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Normalize the pixel values to range 0-1
X_train = X_train.astype('float32') / 255
X_test = X_test.astype('float32') / 255
```

Split the training data in a validation set.
```{python}
from sklearn.model_selection import train_test_split

X_train, X_validate, y_train, y_validate = train_test_split(
    X_train, y_train, test_size=0.15, random_state=42)
```


### Principal Component Analysis (PCA)  
Flatten the images from 32x32 pixels with 3 color channels (n x 32x32x3 array) into a 2-D vector (n x 3072).  
PCA requires a 2-dimensional input (samples x features).
```{python}
# Flatten the X data
X_train_flat = X_train.reshape((X_train.shape[0], -1))
X_validate_flat = X_validate.reshape((X_validate.shape[0], -1))
X_test_flat = X_test.reshape((X_test.shape[0], -1))

print(X_train_flat.shape)
print(X_validate_flat.shape)
print(X_test_flat.shape)
```

PCA was applied to retain 95% of the variance, reducing dimensionality significantly while maintaining image information.
The explained variance plot showed the number of components required to reach this threshold.
Visualization of PCA Impact:
```{python}
from sklearn.decomposition import PCA

# Initialize PCA and fit on the training data
pca = PCA(n_components=0.95)
pca.fit(X_train_flat)

# Transform both the training and testing data
X_train_pca = pca.transform(X_train_flat)
X_validate_pca = pca.transform(X_validate_flat)
X_test_pca = pca.transform(X_test_flat)
```

```{python}
import matplotlib.pyplot as plt
import numpy as np

n_components = pca.n_components_
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot the explained variance
plt.figure(figsize=(8, 4))
plt.plot(cumulative_variance)
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Explained Variance')
plt.grid(True)

# Annotate the number of components used
plt.annotate(f'components: {n_components}', 
             xy=(n_components, cumulative_variance[n_components-1]),  # This places the annotation at the point where the number of components is reached
             xytext=(n_components, cumulative_variance[n_components-1] - 0.10),  # Adjust text position
             ha='center')  # Horizontal alignment

plt.show()
```

Original and PCA-reconstructed images were compared, illustrating that PCA retains substantial image quality despite compression.
Support Vector Machine (SVM) Models:
```{python}
import matplotlib.pyplot as plt

def plot_images(original, reconstructed, n):
    plt.figure(figsize=(10, 4))
    for i in range(n):
        # Plot original images
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(original[i])
        plt.axis('off')
        if i == 0:
            ax.set_title("Original", loc='left')

        # Plot reconstructed images
        ax = plt.subplot(2, n, n + i + 1)
        norm_image = (reconstructed[i] - np.min(reconstructed[i])) / (np.max(reconstructed[i]) - np.min(reconstructed[i]))
        plt.imshow(norm_image)
        plt.axis('off')
        if i == 0:
            ax.set_title("PCA Reconstructed", loc='left')

    plt.show()

# reconstruct the PCA data into 32x32x3 arrays
X_train_reconstructed = pca.inverse_transform(X_train_pca)
X_train_reconstructed = X_train_reconstructed.reshape((X_train.shape[0], 32, 32, 3))
plot_images(X_train, X_train_reconstructed, n=5) # plot first 5 images
```


Load the models and predictions:  
```{python}
import pickle

def load_pickle(path_pkl):
    with open(path_pkl, 'rb') as file:
        pickle_file = pickle.load(file)
    return pickle_file

# load models
model_svm_path = '../model/svm.pkl'
model_svm = load_pickle(model_svm_path)

model_svm_pca_path = '../model/svm_PCA.pkl'
model_svm_pca = load_pickle(model_svm_pca_path)

# load predictions
svm_prediction_path = 'ml_result/test/prediction_svm.pkl'
results_svm = load_pickle(svm_prediction_path)

# test set PCA
svm_pca_prediction_path = 'ml_result/test/prediction_svm_pca.pkl'
results_svm_pca = load_pickle(svm_pca_prediction_path)
```

Calculate performance:  
```{python}
from sklearn.metrics import accuracy_score

# load predictions
preds_svm = load_pickle(svm_prediction_path)
preds_svm_pca = load_pickle(svm_pca_prediction_path)

# calculate accuracy
accuracy_svm = round(accuracy_score(y_test, preds_svm), 3)
print("Accuracy on original data:", accuracy_svm)
accuracy_svm_pca = round(accuracy_score(y_test, preds_svm_pca), 3)
print("Accuracy on PCA data:", accuracy_svm_pca)
```
Accuracy slightly decreases.

```{python}
import time

# Evaluation function
def evaluate_prediction_time(model, X_test, n=100):
    X_test = X_test[:n]
    start_time = time.time()
    model.predict(X_test)
    total_time = time.time() - start_time
    print(f"Total time to predict {n} samples: {total_time:.2f} seconds")

evaluate_prediction_time(model_svm, X_test_flat)
evaluate_prediction_time(model_svm_pca, X_test_pca)
```

Prediction times were evaluated, highlighting the speed advantage of PCA-transformed data.

### Convolutional Neural Network (CNN)

A 9-layer CNN was built, including Conv2D, MaxPooling2D, and Dropout layers.
The CNN was trained and evaluated, achieving a higher accuracy of 0.741 on the test set.
The CNN's prediction time was also evaluated, showing efficient performance.
Conclusion:

While PCA combined with SVM offers dimensionality reduction and faster predictions, CNNs outperform both in accuracy and practical application for image classification tasks.
CNNs' ability to learn complex patterns and adapt to real-time applications makes them superior for modern image processing needs.

Load model:  
```{python}
from tensorflow.keras.models import load_model

cnn_model_path = 'model/cnn.keras'
model_cnn = load_model(cnn_model_path)
```

```{python}
# evaluate accuracy
test_loss, test_accuracy = model_cnn.evaluate(X_test, y_test, verbose=2)
print(f"Test Accuracy: {round(test_accuracy, 3)}")

# evaluate training time
evaluate_prediction_time(model_cnn, X_test)

```

CNNs are superior for image classification due to their ability to learn high-level features directly from data, adapt to complex patterns, and perform efficiently in real-time applications, including on IoT devices. While PCA + SVM might still be used for less complex or highly specific tasks where training data is limited or when computational simplicity is prioritized, CNNs are the standard for most current image processing tasks. This difference is crucial in educational settings, emphasizing the need to train students on technologies that they are most likely to use in industry, particularly in cutting-edge fields like machine learning and computer vision.

This comparison illustrates the importance of using appropriate techniques for specific tasks. PCA with SVM may still be useful for simpler tasks or where computational resources are limited, but CNNs are the gold standard for most current image classification tasks.

## Conclusion  

In summary, Principal Component Analysis has proven invaluable in reducing the dimensionality of complex datasets, allowing for clearer visualization and interpretation of the underlying structures. This technique transforms original variables into uncorrelated principal components, effectively capturing the most significant patterns in the data while mitigating issues such as multicollinearity and overfitting. Despite its limitations, including sensitivity to outliers and potential information loss from nonlinear relationships, PCA remains a crucial tool in various applications. These include image compression, signal processing, and machine learning pipelines, resulting in improved model performance and computational efficiency.

Applying PCA to a sample dataset, which includes demographic, health, and environmental metrics for counties in the deep south of the United States, revealed that the first four principal components explain 72% of the variance. PC1 is heavily loaded with health and socioeconomic variables, indicating a general health status dimension. PC2 captures environmental pollution factors, specifically emissions of mercury and lead, while PC3 reflects additional health and socioeconomic complexities. This analysis underscores the ability of PCA to distill vast amounts of information into comprehensible insights, facilitating more effective data-driven decision-making.

