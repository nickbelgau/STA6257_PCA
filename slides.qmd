---
title: "Practical Implementations of Principal Component Analysis"
author: "Nick Belgau, Oscar Hernandez Mata"
date: "2024-08-01"
format:
  revealjs:
    toc: true
    smaller: true
    progress: true
    slide-number: true
    width: 1280
    height: 960

    
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
self-contained: true
execute: 
  echo: true
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


## The Math behind PCA  

::: {.panel-tabset}

### Why SVD is widely used 

**SVD is faster and more accurate than eigen-decomposition**  

Although PCA is traditionally taught through eigen-decomposition of the covariance matrix, Singular Value Decomposition (SVD) is always used in practice.

- Numerical stability: No need to square the covariance matrix ($X^TX$) which can amplify errors; robust against ill-conditioned matrices.
- Efficient with large datasets: Directly decomposes the data without needing to compute and square the covariance matrix [@johnson2023applied].  


:::: {.columns}  

::: {.column width="50%" style="text-align: center;"}
![](figures/sklearn_logo.png){style="height: 150px; object-fit: cover;"}  

sklearn.decomposition.PCA
:::

::: {.column width="50%" style="text-align: center;"}
![](figures/r_logo.jpeg){style="height: 150px; object-fit: cover;"}  

stats::prcomp()
:::

::::

### Overview of SVD algorithm
**SVD decomposition**  
Ensure features are continuous and standardized ($\mu$ = 0, $\sigma$ = 1):   
$$
X = U \Sigma V^T
$$
Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other and construct the new axes of maximum variance. 



**Calculate explained variance by PC**  
The diagonal singular value matrix $\Sigma$ corresponds to the strength of each PC:  
$$
\text{variance_explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
$$

**Dimensionality reduction:**  
- Select PCs: Based on cumulative explained variance target (95%).  
- Truncate $V$: Select top PCs to reduce dimensions and transform $X$ into a new feature space.
  $$
  X_{\text{transformed}} = X V_{\text{selected}}
  $$


### Overview of Assumptions  

The effectiveness of PCA relies on satisfying these points.  

1. **Linearity**  
PCA assumes that resulting principal components are linear combinations of the original variables. Nonlinear relationships may lead to low covariance values which can lead to an undervalued representation of their signficance.  

2. **Continuous data**  
PCA begins by standardizing the data, so the features should come from continuous distribution. Because the scale of measurement conveys information about the variance, categorical variables should be handled separately.  

3. **Data standardization**  
- Scaling standardizes the variance of each variable to ensure equal contributions.  
- Mean-centering has a similar impact: ensuring that the principal components capture the true direction of maximum variance.   
- Outliers can also distort the PCs, so they should be identified and handled appropriately.   

:::

---

## Application 1 - Demographic Data

```{r}
#| echo: false
library(readxl)
library(skimr)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(corrplot)
library(DescTools)
library(reshape2)
library(factoextra)
library(kableExtra)
library(rrcov)
library(psych)
library(stats)
library(ggfortify)
library(lmtest)
library(car)

url <- "https://raw.githubusercontent.com/nickbelgau/STA6257_PCA/main/data/demographic/Alz_mortality_data_complete_with_state_csv.csv"
data_raw <- read_csv(url)

selected_columns <- c(
  "County", "State", "obesity_age_adj", "Smoking_Rate", "Diabetes", "Heart_Disease", "Cancer",  "Mercury_TPY", "Lead_TPY", "Food_index", "Poverty_Percent", "Atrazine_High_KG", "SUNLIGHT"
)

deep_south_states <- c("AL", "AR", "FL", "GA", "LA", "MS", "NC", "SC", "TN", "TX", "VA")

data <- data_raw %>%
  filter(State %in% deep_south_states) %>%
  select(all_of(selected_columns))

data <- data %>%
  select(-County, -State)
```

### Dataset Description

This application was inspired by a paper published by UWF (Amin, Yacko, and Guttmann) on Alzheimer's disease mortality [@tejada_vera_2013] [@amin_2018].

The dataset is derived from US census data and contains demographic, health, and environmental metrics for counties in the United States. The dataset was filtered to select counties in the deep south.  

::: {.column width="70%"}
 
![](figures/highlighted_states.png)  

:::

::: {.column width="30%"}

| **Column**          |
|---------------------|
| Obesity Age Adj     |
| Smoking Rate        |
| Diabetes            |
| Heart Disease       |
| Cancer              | 
| Food Index          |
| Poverty Percent     |
| Physical Inactivity |
| Mercury TPY         |
| Lead TPY            |
| Atrazine High KG    |

:::


---

## Application 1 - Demographic Data {.scrollable}  
### Check Assumptions: Continuous Variables  

- The variables appear to be continuous because the data types are "numeric" with high cardinality.  
- There are no nulls.  
- It is clear that scaling and mean-centering will be needed.  

```{r}
skim(data)
```

---

## Application 1 - Demographic Data 
### Check Assumptions: Linarity Analysis  

The Harvey-Collier Test can automate this by running pairwise test for linearity, and if a significant nonzero slope on the residuals exists, the p-value will be less than alpha [@nwakuya2022instability] [@harvey1977testing]. 

**Visually checking scatter plots is not a realistic method for inspecting linearity in real-world applications. Correlation plots are insufficient.**  

```{r}
#| code-fold: true
harvey_collier_test <- function(data, x, y) {
  formula <- as.formula(paste(y, "~", x))
  model <- lm(formula, data = data)
  test <- harvtest(model)
  p_value <- test$p.value
  return(signif(p_value, digits = 2))
}

variables <- names(data)
n <- length(variables)
p_matrix <- matrix(NA, n, n, dimnames = list(variables, variables))

for (i in 1:n) {
  for (j in 1:n) {
    if (i != j) {  # Avoid testing a variable against itself
      p_matrix[i, j] <- harvey_collier_test(data, variables[i], variables[j])
    }
  }
}
```

<img src="figures/harvey_collier_heatmap.png" width="60%" height="auto"/>


---

## Application 1 - Demographic Data 
### Check Assumptions: Linarity Analysis (continued) 

- The residual plot for a single pair that was flagged as nonlinear reveals why the Harvey-Collier Test declared nonlinearity.  
- Transformations may risk altering other variable relationships, so no transformations were applied, acknowledging some information loss in PCA.

```{r}
#| code-fold: true
data_residual <- as.data.frame(data)

data_residual$residuals <- residuals(lm(Diabetes ~ obesity_age_adj, data = data_residual))

ggplot(data_residual, aes(x = obesity_age_adj, y = residuals)) +
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE, color = "blue") +  # LOWESS curve
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals of Diabetes vs. obesity_age_adj",
       x = "obesity_age_adj", y = "Residuals") +
  theme_minimal()
```

---

## Application 1 - Demographic Data 
### Check Assumptions: Outliers Analysis  

**note: the mean-centering and scaling is handled within the PCA implementation**  

- Outliers can distort PCA results by disproportionately increasing variance, shifting the direction of principal components, and inflating eigenvalues.  
- Although PCA does not require normality, a roughly normal distribution minimizes the impact from outliers.  
- Non-normal data may undergo transformations like the Box-Cox to approximate normality.  
- Assessing skewness and kurtosis offers practical insights into distribution characteristics.

::: {.column width="35%"}  

```{r}
#| echo: false
library(moments)

distribution_metrics <- function(df) {
  results <- data.frame(
    Kurtosis = sapply(data, kurtosis),
    Skewness = sapply(data, skewness)
  )
  results <- results[order(-results$Kurtosis),]
  return(results)
}

print(distribution_metrics(data))
```

:::

::: {.column width="65%"}  

```{r}
#| echo: false
columns_to_transform <- c("Lead_TPY", "Mercury_TPY", "Atrazine_High_KG")

par(mfrow=c(3, 1), mai=c(0.5, 0.8, 0.5, 0.5))  # Adjusted margins

for (col in columns_to_transform) {
  boxplot(
    data[[col]],
    horizontal=TRUE,
    main=paste("Boxplot of", col),
    col="lightblue",
    border="darkblue",
    las=1  # Optional: Makes labels horizontal
  )
}
```

:::

---

## Application 1 - Demographic Data 
### Check Assumptions: Outliers Analysis (continued) 

- The Box-Cox transformation can normalize the data distribution, enhancing its suitability for PCA [@compression2014boxcox].  
- The transformations create an approximate bell-shaped distribution, reducing outlier effects.  
- The lambda values indicate minor power adjustments.  

::: {.column width="80%"}  

```{r}
#| echo: false
library(MASS)

box_cox_transform <- function(df, columns) {
  transformed_df <- df
  lambdas <- list()
  for (col in columns) {
    col_data <- df[[col]]
    col_data[col_data <= 0] <- min(col_data[col_data > 0]) / 2
    bc <- boxcox(col_data ~ 1, plotit=FALSE)
    lambda <- bc$x[which.max(bc$y)]
    transformed_df[[col]] <- (col_data^lambda - 1) / lambda
    lambdas[[col]] <- lambda
  }
  return(list(transformed_df, lambdas))
}

result <- box_cox_transform(data, columns_to_transform)
data_transform <- result[[1]]
lambdas <- result[[2]]

par(mfrow = c(length(columns_to_transform) + 1, 1), mar = c(4, 4, 2, 2))
for (i in columns_to_transform) {
  hist(data_transform[[i]], probability = TRUE, main = paste("Density Plot of", i), xlab = "Values", col = "lightblue", border = "darkblue")
  lines(density(data_transform[[i]]), col = "darkred", lwd = 2)
}
```

:::

::: {.column width="20%"}  

```{r}
#| echo: false
lambda_df <- data.frame(
  Lambda = unlist(lambdas)
)
print(lambda_df)
```

:::

---

## Application 1 - Demographic Data 
### Correlation Analysis  
 
- While not a complete diagnosis, identifying highly correlated variables can indicate multicollinearity within the dataset (VIF will be looked at later).  
- This provides insights to the effectiveness of PCA because correlated variables can be transformed into orthogonal components which eliminate multicollinearity.  

```{r}
#| echo: false
library(corrplot)

cor_matrix <- cor(data, use = "complete.obs")  # Handle missing values

color_palette <- colorRampPalette(c("#215B9D", "#DCE6F1", "#215B9D"))(200) # blue #215B9D

corrplot(abs(cor_matrix), method = "color",
        #  type = "lower", 
         order = "hclust",
         addCoef.col = "#36454F",
         number.cex = 0.50,
         tl.col = "black",
         tl.srt = 45,  # No rotation for text labels
        #  tl.pos = "d",  # Position text labels at the bottom (x-axis)
         cl.pos="n",
         col = color_palette,
         bg = "white"
)
```

---