---
title: "Practical Implementations of Principal Component Analysis"
author: "Nick Belgau, Oscar Hernandez Mata"
date: "2024-08-01"
format:
  revealjs:
    width: 1280
    height: 720
    toc: true
    smaller: true
    progress: true
    slide-number: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
self-contained: true
execute: 
  echo: true
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


## The Math behind PCA  

::: {.panel-tabset}

### Why SVD is widely used 

**SVD is faster and more accurate than eigen-decomposition**  

Although PCA is traditionally taught through eigen-decomposition of the covariance matrix, Singular Value Decomposition (SVD) is always used in practice.

- Numerical stability: No need to square the covariance matrix ($X^TX$) which can amplify errors; robust against ill-conditioned matrices.
- Efficient with large datasets: Directly decomposes the data without needing to compute and square the covariance matrix [@johnson2023applied].  


:::: {.columns}  

::: {.column width="50%" style="text-align: center;"}
![](slide_figures/sklearn_logo.png){style="height: 150px; object-fit: cover;"}  

sklearn.decomposition.PCA
:::

::: {.column width="50%" style="text-align: center;"}
![](slide_figures/r_logo.jpeg){style="height: 150px; object-fit: cover;"}  

stats::prcomp()
:::

::::

### Overview of SVD algorithm
**SVD decomposition**  
Ensure features are continuous and standardized ($\mu$ = 0, $\sigma$ = 1):   
$$
X = U \Sigma V^T
$$
Each column of $ùëâ$ represents a principal component (PC) which are orthogonal to each other and construct the new axes of maximum variance. 



**Calculate explained variance by PC**  
The diagonal singular value matrix $\Sigma$ corresponds to the strength of each PC:  
$$
\text{variance_explained} = \frac{\sigma_i^2}{\sum \sigma_i^2}
$$

**Dimensionality reduction:**  
- Select PCs: Based on cumulative explained variance target (95%).  
- Truncate $V$: Select top PCs to reduce dimensions and transform $X$ into a new feature space.
  $$
  X_{\text{transformed}} = X V_{\text{selected}}
  $$


### Overview of Assumptions  

The effectiveness of PCA relies on satisfying these points.  

1. **Linearity**  
PCA assumes that resulting principal components are linear combinations of the original variables. Nonlinear relationships may lead to low covariance values which can lead to an undervalued representation of their signficance.  

2. **Continuous data**  
PCA begins by standardizing the data, so the features should come from continuous distribution. Because the scale of measurement conveys information about the variance, categorical variables should be handled separately.  

3. **Data standardization**  
- Scaling standardizes the variance of each variable to ensure equal contributions.  
- Mean-centering has a similar impact: ensuring that the principal components capture the true direction of maximum variance.   
- Outliers can also distort the PCs, so they should be identified and handled appropriately.   

:::

---

## Application 1 - Tabular Demographic Data

```{r}
#| echo: false
library(readxl)
library(skimr)
library(dplyr)
library(readr)
library(ggplot2)
library(GGally)
library(corrplot)
library(DescTools)
library(reshape2)
library(factoextra)
library(kableExtra)
library(rrcov)
library(psych)
library(stats)
library(ggfortify)
library(lmtest)
library(car)
```

### Dataset Description

The dataset contains demographic, health, and environmental metrics for counties in the United States from US census data. Each row represents data for a specific county of a state [@tejada_vera_2013] [@amin_2018]. The columns analyzed were:  
- obesity_age_adj: Age-adjusted obesity rate.  
- Smoking_Rate: Rate of smoking within the population.  
- Diabetes: Diabetes prevalence rate.  
- Heart_Disease: Heart disease prevalence rate.  
- Cancer: Cancer prevalence rate.  
- Food Index: Index score representing food availability and quality.  
- Poverty_Percent: Percentage of the population living below the poverty line.  
- physical_inactivity: Rate of physical inactivity.  
- Mercury_TPY: Mercury emissions in tons per year.  
- Lead_TPY: Lead emissions in tons per year.  
- Atrazine_High_KG: Atrazine in KG per year.  

```{r}
url <- "https://raw.githubusercontent.com/nickbelgau/STA6257_PCA/main/data/demographic/Alz_mortality_data_complete_with_state_csv.csv"
data_raw <- read_csv(url)

selected_columns <- c(
  "County", "State", "obesity_age_adj", "Smoking_Rate", "Diabetes", "Heart_Disease", "Cancer",  "Mercury_TPY", "Lead_TPY", "Food_index", "Poverty_Percent", "Atrazine_High_KG", "SUNLIGHT"
)

deep_south_states <- c("AL", "AR", "FL", "GA", "LA", "MS", "NC", "SC", "TN", "TX", "VA")

data <- data_raw %>%
  filter(State %in% deep_south_states) %>%
  select(all_of(selected_columns))

data <- data %>%
  select(-County, -State)
```


The dataset was filtered to analyze only selected variables for states in the deep south in the United States. The county and state columns were removed, creating 1 entire region on the dataset. Columns were selected to remove demographic fields related to gender, sex, and race and ambigious fields related to grouping.


The variables appear to be continuous because the data types are "numeric" with high cardinality. There are no nulls. It is clear that scaling and mean-centering will be needed.  

```{r}
skim(data)
```


---



### Linarity Analysis  
PCA works on the premise that principal components are linear combinations of the original features. Linearity between variables boosts PCA efficiency, while its absence may cause information loss. Statistical pairwise testing can automate the identification of non-linear relationships, providing a practical alternative to time-intensive visual inspection methods which is particularly helpful for large datasets.  

The Harvey-Collier Test for Linearity fits a linear model to each variable pair, calculates recursive residuals, and conducts regression again on these residuals [@nwakuya2022instability] [@harvey1977testing]. A significant nonzero slope in the residual regression indicates non-linearity, suggesting potential information loss in PCA.

```{r}
harvey_collier_test <- function(data, x, y) {
  formula <- as.formula(paste(y, "~", x))
  model <- lm(formula, data = data)
  test <- harvtest(model)
  p_value <- test$p.value
  return(signif(p_value, digits = 2))
}

variables <- names(data)
n <- length(variables)
p_matrix <- matrix(NA, n, n, dimnames = list(variables, variables))

for (i in 1:n) {
  for (j in 1:n) {
    if (i != j) {  # Avoid testing a variable against itself
      p_matrix[i, j] <- harvey_collier_test(data, variables[i], variables[j])
    }
  }
}
```
When the p-value is less than the signficance level, the test declares that the relationship is nonlinear. A heatmap of the p-values reveals which pairs are nonlinear with color-coding. 
```{r}
library(ggplot2)

p_matrix_long <- melt(p_matrix)
names(p_matrix_long) <- c("Var1", "Var2", "p_value")

p_matrix_long$Var1 <- factor(p_matrix_long$Var1, levels = rev(unique(p_matrix_long$Var1)))

alpha = 0.0001

gradient_fill <- scale_fill_gradientn(
  colors = c("#215B9D", "#F0F0F0", "#F0F0F0"),
  values = scales::rescale(c(0, alpha, 1)),
  na.value = "#F0F0F0",  # Also set missing values to light grey
  guide = "colourbar"
)

ggplot(p_matrix_long, aes(Var1, Var2, fill= p_value)) + 
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2e", p_value)), color = "black", size = 2) + 
  gradient_fill +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") + 
  labs(title = "Heatmap of P-Values") 
```

The residual plot for a single pair that was flagged as nonlinear reveals why the Harvey-Collier Test declared nonlinearity. Although transformations can adjust linearity, they risk altering other variable relationships. Thus, no transformations were applied, acknowledging some information loss in PCA.

```{r}
data_residual <- as.data.frame(data)

data_residual$residuals <- residuals(lm(Diabetes ~ obesity_age_adj, data = data_residual))

ggplot(data_residual, aes(x = obesity_age_adj, y = residuals)) +
  geom_point() + 
  geom_smooth(method = "loess", se = FALSE, color = "blue") +  # LOWESS curve
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals of Diabetes vs. obesity_age_adj",
       x = "obesity_age_adj", y = "Residuals") +
  theme_minimal()
```

---

### Outliers Analysis  
Outliers can distort PCA results by disproportionately increasing variance, shifting the direction of principal components, and inflating eigenvalues. Before implementing outlier removal techniques, it is crucial to first examine the distribution to validate the rationale behind outlier exclusion. Although PCA does not require normality, a roughly normal distribution minimizes the impact from outliers. Non-normal data may undergo transformations like the Box-Cox to approximate normality. Assessing skewness and kurtosis offers practical insights into distribution characteristics.

```{r}
library(moments)

distribution_metrics <- function(df) {
  results <- data.frame(
    Kurtosis = sapply(data, kurtosis),
    Skewness = sapply(data, skewness)
  )
  results <- results[order(-results$Kurtosis),]
  return(results)
}

print(distribution_metrics(data))
```
This analysis reveals columns with high kurtosis and skewness. Boxplots confirm that these are variables are right-skewed with numerous outliers, suggesting the need for transformation to improve PCA.
```{r}
columns_to_transform <- c("Lead_TPY", "Mercury_TPY", "Atrazine_High_KG")

par(mfrow=c(3, 1)) # format
for (col in columns_to_transform) {
  boxplot(
    data[[col]],
    horizontal=TRUE,
    main=paste("Boxplot of", col),
    col="lightblue",
    border="darkblue"
  )
}
```

The Box-Cox transformation is a robust statistical method that normalizes the data distribution, enhancing its suitability for PCA [@compression2014boxcox]. It is considered best practice to apply such transformations to columns selectively rather than on the entirety of the dataset. Automating this process can be achieved by setting a kurtosis threshold.Additionally, negative values must be carefully managed to ensure the Box-Cox transformation is correctly implemented. 
```{r}
library(MASS)

box_cox_transform <- function(df, columns) {
  transformed_df <- df
  lambdas <- list()
  for (col in columns) {
    col_data <- df[[col]]
    col_data[col_data <= 0] <- min(col_data[col_data > 0]) / 2
    bc <- boxcox(col_data ~ 1, plotit=FALSE)
    lambda <- bc$x[which.max(bc$y)]
    transformed_df[[col]] <- (col_data^lambda - 1) / lambda
    lambdas[[col]] <- lambda
  }
  return(list(transformed_df, lambdas))
}

result <- box_cox_transform(data, columns_to_transform)
data_transform <- result[[1]]
lambdas <- result[[2]]

par(mfrow = c(length(columns_to_transform) + 1, 1), mar = c(4, 4, 2, 2))
for (i in columns_to_transform) {
  hist(data_transform[[i]], probability = TRUE, main = paste("Density Plot of", i), xlab = "Values", col = "lightblue", border = "darkblue")
  lines(density(data_transform[[i]]), col = "darkred", lwd = 2)
}
```
The transformations create an approximate bell-shaped distribution, reducing outlier effects. Minor power adjustments, indicated by lambda values, enhance PCA effectiveness without the need for manual outlier removal.  
```{r}
lambda_df <- data.frame(
  Lambda = unlist(lambdas)
)
print(lambda_df)
```

---

### Correlation Analysis
A heatmap of the correlation matrix was analyzed to identify highly correlated variables, indicating potential multicollinearity within the dataset. By applying PCA, correlated variables can be transformed into orthogonal components which eliminate multicollinearity. This transformation ensures each principal component captures unique variance, improving the clarity and reliability of modeling.

```{r}
library(corrplot)

cor_matrix <- cor(data, use = "complete.obs")  # Handle missing values

color_palette <- colorRampPalette(c("#215B9D", "#DCE6F1", "#215B9D"))(200) # blue #215B9D

corrplot(abs(cor_matrix), method = "color",
        #  type = "lower", 
         order = "hclust",
         addCoef.col = "#36454F",
         number.cex = 0.50,
         tl.col = "black",
         tl.srt = 45,  # No rotation for text labels
        #  tl.pos = "d",  # Position text labels at the bottom (x-axis)
         cl.pos="n",
         col = color_palette,
         bg = "white"
)
```
